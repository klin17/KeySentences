{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changing all files in /Data/cnn/stories with a .story extension to use a .txt extension instead...\n",
      "92579 files in directory\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "#from change_extensions import *\n",
    "#from baseline import *\n",
    "#from data_cleaning import *\n",
    "#from extract_stories import *\n",
    "#from testing import *\n",
    "#import Data\n",
    "#Data/cnn/stories\n",
    "%run change_extensions Data/cnn/stories .story .txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 92579 files\n",
      "Finished 2314 files: 2314/92579 = 2.499486924680543%, in 148.23329901695251 seconds\n",
      "Finished 4628 files: 4628/92579 = 4.998973849361086%, in 144.02052474021912 seconds\n",
      "Finished 6942 files: 6942/92579 = 7.49846077404163%, in 142.00053310394287 seconds\n",
      "Finished 9256 files: 9256/92579 = 9.997947698722172%, in 142.37709403038025 seconds\n",
      "Finished 11570 files: 11570/92579 = 12.497434623402716%, in 148.56068086624146 seconds\n",
      "Finished 13884 files: 13884/92579 = 14.99692154808326%, in 149.5020089149475 seconds\n",
      "Finished 16198 files: 16198/92579 = 17.4964084727638%, in 161.79482650756836 seconds\n",
      "Finished 18512 files: 18512/92579 = 19.995895397444343%, in 158.63236451148987 seconds\n",
      "Finished 20826 files: 20826/92579 = 22.49538232212489%, in 160.34246850013733 seconds\n",
      "Finished 23140 files: 23140/92579 = 24.994869246805433%, in 152.88791394233704 seconds\n",
      "Finished 25454 files: 25454/92579 = 27.494356171485975%, in 157.29456281661987 seconds\n",
      "Finished 27768 files: 27768/92579 = 29.99384309616652%, in 157.76622915267944 seconds\n",
      "Finished 30082 files: 30082/92579 = 32.49333002084706%, in 157.04414868354797 seconds\n",
      "Finished 32396 files: 32396/92579 = 34.9928169455276%, in 156.0224893093109 seconds\n",
      "Finished 34710 files: 34710/92579 = 37.49230387020815%, in 162.27086687088013 seconds\n",
      "Finished 37024 files: 37024/92579 = 39.991790794888686%, in 160.04881501197815 seconds\n",
      "Finished 39338 files: 39338/92579 = 42.49127771956923%, in 162.2513725757599 seconds\n",
      "Finished 41652 files: 41652/92579 = 44.99076464424978%, in 165.7060296535492 seconds\n",
      "Finished 43966 files: 43966/92579 = 47.49025156893032%, in 172.7582540512085 seconds\n",
      "Finished 46280 files: 46280/92579 = 49.989738493610865%, in 163.73481249809265 seconds\n",
      "Finished 48594 files: 48594/92579 = 52.489225418291404%, in 163.6716823577881 seconds\n",
      "Finished 50908 files: 50908/92579 = 54.98871234297195%, in 164.3481903076172 seconds\n",
      "Finished 53222 files: 53222/92579 = 57.48819926765249%, in 171.71475291252136 seconds\n",
      "Finished 55536 files: 55536/92579 = 59.98768619233304%, in 161.34699058532715 seconds\n",
      "Finished 57850 files: 57850/92579 = 62.487173117013576%, in 162.3059995174408 seconds\n",
      "Finished 60164 files: 60164/92579 = 64.98666004169412%, in 160.99952602386475 seconds\n",
      "Finished 62478 files: 62478/92579 = 67.48614696637466%, in 163.68994641304016 seconds\n",
      "Finished 64792 files: 64792/92579 = 69.9856338910552%, in 159.90086936950684 seconds\n",
      "Finished 67106 files: 67106/92579 = 72.48512081573575%, in 180.76756048202515 seconds\n",
      "Finished 69420 files: 69420/92579 = 74.9846077404163%, in 182.89161729812622 seconds\n",
      "Finished 71734 files: 71734/92579 = 77.48409466509683%, in 162.37238812446594 seconds\n",
      "Finished 74048 files: 74048/92579 = 79.98358158977737%, in 165.36105942726135 seconds\n",
      "Finished 76362 files: 76362/92579 = 82.48306851445793%, in 176.65078139305115 seconds\n",
      "Finished 78676 files: 78676/92579 = 84.98255543913847%, in 174.22071051597595 seconds\n",
      "Finished 80990 files: 80990/92579 = 87.482042363819%, in 160.5469102859497 seconds\n",
      "Finished 83304 files: 83304/92579 = 89.98152928849956%, in 160.4824140071869 seconds\n",
      "Finished 85618 files: 85618/92579 = 92.4810162131801%, in 159.08205270767212 seconds\n",
      "Finished 87932 files: 87932/92579 = 94.98050313786064%, in 156.00691938400269 seconds\n",
      "Finished 90246 files: 90246/92579 = 97.47999006254118%, in 156.73060703277588 seconds\n",
      "Finished 92560 files: 92560/92579 = 99.97947698722173%, in 158.40925431251526 seconds\n",
      "Produced 22651 files with key sentences\n",
      "Failed on 69928 files\n",
      "24.466671707406647% failure rate\n",
      "75.53332829259335% failure rate\n",
      "total time: 6425.176290512085 seconds\n"
     ]
    }
   ],
   "source": [
    "%run data_cleaning.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "#import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "#doc will be in csv form, gets labels and raw sentences\n",
    "def get_doc_raw_data(lab_doc_path):\n",
    "    sentences = []\n",
    "    labels = []\n",
    "    with open(lab_doc_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        for line in reader:\n",
    "            sentence = line[\"sentence\"]\n",
    "            label = line[\"label\"]\n",
    "            sentences.append(sentence)\n",
    "            labels.append(label)\n",
    "    return [sentences, labels]         \n",
    "\n",
    "def get_cosine_sim(a, b):\n",
    "    return dot(a, b)/(norm(a)*norm(a))\n",
    "\n",
    "def get_list_for_cont_n(sent_embeds, n):\n",
    "    to_ret = []\n",
    "    for i in range(len(sent_embeds)):\n",
    "        if (i - n) < 0:\n",
    "            low_bound = 0\n",
    "        else:\n",
    "            low_bound = i-n\n",
    "        if (i + n) > (len(sent_embeds) - 1):\n",
    "            up_bound = len(sent_embeds) -1\n",
    "        else:\n",
    "            up_bound = i + n\n",
    "        cont_window = sent_embeds[low_bound:(up_bound+1)]\n",
    "        cont_window_vect = np.mean(cont_window, axis = 0).tolist()\n",
    "        to_ret.append(get_cosine_sim(cont_window_vect, sent_embeds[i]))\n",
    "    return to_ret\n",
    "        \n",
    "def get_doc_LR_inputs(lab_doc_path):\n",
    "    doc_raw_data = get_doc_raw_data(lab_doc_path)\n",
    "    raw_sents = doc_raw_data[0]\n",
    "    raw_labels = doc_raw_data[1]\n",
    "    #vector_sents_list = [nlp(sent).vector.reshape(1,-1) for sent in raw_sents]\n",
    "    nlp_sents_list = list(nlp.pipe(raw_sents))\n",
    "    embed_sents_list = [nlp_sent.vector.tolist() for nlp_sent in nlp_sents_list]\n",
    "    num_n_ents_list = [len(nlp_sent.ents) for nlp_sent in nlp_sents_list]\n",
    "    sent_lengths_bychar_list = [len(raw_sent) for raw_sent in raw_sents]\n",
    "    sent_lengths_list = [len(nlp_sent) for nlp_sent in nlp_sents_list]\n",
    "    \n",
    "    sent_char_tok_ratio_list = []\n",
    "    #print(type)\n",
    "    for i in range(len(raw_sents)):\n",
    "        sent_char_tok_ratio_list.append(sent_lengths_bychar_list[i]/sent_lengths_list[i])\n",
    "    \n",
    "    doc_vector = np.mean(embed_sents_list, axis = 0).tolist()\n",
    "    doc_sim_list = [get_cosine_sim(doc_vector, sent_vector) for sent_vector in embed_sents_list]\n",
    "    \n",
    "    cont_winds_lists = []\n",
    "    cont_winds_to_do = [1, 2, 3, 4, 5]\n",
    "    \n",
    "    for n in cont_winds_to_do:\n",
    "        cont_winds_lists.append(get_list_for_cont_n(embed_sents_list, n))\n",
    "        \n",
    "    embed_feats = np.array(embed_sents_list).T.tolist()\n",
    "    \n",
    "    feats = []\n",
    "    feats.append(num_n_ents_list)\n",
    "    feats.append(sent_lengths_list)\n",
    "    feats.append(sent_lengths_bychar_list)\n",
    "    feats.append(sent_char_tok_ratio_list)\n",
    "    feats.append(doc_sim_list)\n",
    "    for listt in cont_winds_lists:\n",
    "        feats.append(listt)\n",
    "    \n",
    "    dim_to_inc = []\n",
    "    for i in range(len(embed_feats)):\n",
    "        dim_to_inc.append(i+1)\n",
    "        \n",
    "    for i in range(len(embed_feats)):\n",
    "        if (i+1) in dim_to_inc:\n",
    "            feats.append(embed_feats[i])\n",
    "            \n",
    "    vector_sents_matrix = np.array(feats).T.tolist()\n",
    "    \n",
    "    #print(vector_sents_matrix[0])\n",
    "    \n",
    "    #vector_sents_matrix = np.concatenate(vector_sents_list)\n",
    "    return [raw_labels, vector_sents_matrix]\n",
    "\n",
    "#testing on single doc\n",
    "def LR_on_doc(lab_doc_path):\n",
    "    inputs = get_doc_LR_inputs(lab_doc_path)\n",
    "    labels = inputs[0]\n",
    "    sents_matrix = inputs[1]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(sents_matrix, labels, random_state=42)\n",
    "    LR = LogisticRegression(multi_class='multinomial', solver='lbfgs', penalty='l2', fit_intercept=False)\n",
    "    LR.fit(X_train, y_train)\n",
    "    pred_labels = LR.predict(X_test)\n",
    "    actual_labels = y_test\n",
    "    overall_test_results = []\n",
    "    for i in range(len(pred_labels)):\n",
    "        overall_test_results.append({'Number': i, 'pred': pred_labels[i], 'act': actual_labels[i]})\n",
    "    for result in overall_test_results:\n",
    "        print(result)\n",
    "        print()\n",
    "        \n",
    "def LR_on_first_n(dir_name, n, train_set):\n",
    "    raw_file_names = os.listdir(dir_name)[0:n]\n",
    "    LR_x_inputs = []\n",
    "    LR_labels = []\n",
    "    \n",
    "    X_train = []\n",
    "    X_test = []\n",
    "    y_train = []\n",
    "    y_test = []\n",
    "    c = 0\n",
    "    for raw_name in raw_file_names:\n",
    "        file_path = os.path.join(\"labeled_data2_0.8\", raw_name)\n",
    "        doc_LR_inputs = get_doc_LR_inputs(file_path)\n",
    "        LR_x_inputs.append(doc_LR_inputs[1])\n",
    "        LR_labels.append(doc_LR_inputs[0])\n",
    "        if c in train_set:\n",
    "            X_train.append(doc_LR_inputs[1])\n",
    "            y_train.append(doc_LR_inputs[0])\n",
    "        else:\n",
    "            X_test.append(doc_LR_inputs[1])\n",
    "            y_test.append(doc_LR_inputs[0])\n",
    "        c += 1\n",
    "        \n",
    "    #X_train, X_test, y_train, y_test = train_test_split(LR_x_inputs, LR_labels, random_state=42)\n",
    "    \n",
    "    \n",
    "    X_train_list = []\n",
    "    for doc_matrix in X_train:\n",
    "        for sent_vector in doc_matrix:\n",
    "            X_train_list.append(sent_vector)\n",
    "    X_train_matrix = X_train_list\n",
    "    #X_train_matrix = np.concatenate(X_train_list)\n",
    "    y_train_list = []\n",
    "    for doc in y_train:\n",
    "        for label in doc:\n",
    "            y_train_list.append(label)\n",
    "    LR = LogisticRegression(multi_class='multinomial', fit_intercept=False, solver='lbfgs', penalty='l2')\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train_matrix = scaler.fit_transform(X_train_matrix)\n",
    "    #X_test = scaler.transform(X_test)\n",
    "    LR.fit(X_train_matrix, y_train_list)\n",
    "    pred_labels = []\n",
    "    act_labels = []\n",
    "    num_sentences = 0\n",
    "    num_act_true = 0\n",
    "    num_pred_true = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    tp = 0\n",
    "    tn = 0\n",
    "    \n",
    "    for i in range(len(X_test)):\n",
    "        X_test[i] = scaler.transform(X_test[i])\n",
    "        pred_labels.append(LR.predict(X_test[i]))\n",
    "        act_labels.append(y_test[i])\n",
    "    for i in range(len(pred_labels)):\n",
    "        #print('Next doc tests')\n",
    "        otr = []\n",
    "        for c in range(len(pred_labels[i])):\n",
    "            curr_pred = pred_labels[i][c]\n",
    "            curr_act = act_labels[i][c]\n",
    "            if(curr_pred == 'True' and curr_act == 'True'):\n",
    "                tp += 1\n",
    "            elif(curr_pred == 'True' and curr_act == 'False'):\n",
    "                fp += 1\n",
    "            elif(curr_pred == 'False' and curr_act == 'True'):\n",
    "                fn += 1\n",
    "            elif(curr_pred == 'False' and curr_act == 'False'):\n",
    "                tn += 1\n",
    "            if(curr_pred == 'True'):\n",
    "                num_pred_true += 1\n",
    "            if(curr_act == 'True'):\n",
    "                num_act_true += 1\n",
    "            num_sentences += 1\n",
    "            otr.append({'Number': c, 'pred': curr_pred, 'act': curr_act})\n",
    "        #for res in otr:\n",
    "            #print(res)\n",
    "        #print()\n",
    "    #coefs = LR.coef_\n",
    "    #print(coefs[0])\n",
    "    print(num_sentences)\n",
    "    print(num_act_true)\n",
    "    print(num_pred_true)\n",
    "    print(fp)\n",
    "    print(fn)\n",
    "    print(tp)\n",
    "    print(tn)\n",
    "    return [pred_labels, act_labels, LR.coef_]\n",
    "#def get_train_set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "185567\n",
      "7277\n",
      "87504\n",
      "81759\n",
      "1532\n",
      "5745\n",
      "96531\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "n = 22651\n",
    "split_index = math.floor(.75 * n)\n",
    "dir_name = \"labeled_data2_0.8\"\n",
    "raw_name = os.listdir(dir_name)[100]\n",
    "#story_file = os.path.join(\"stories\", raw_name + \".txt\")\n",
    "labeled_file = os.path.join(\"labeled_data2_0.8\", raw_name)\n",
    "#from Log_Reg_Rel_Functions import *\n",
    "#LR_on_doc(labeled_file)\n",
    "train_set = []\n",
    "\n",
    "for i in range(split_index):\n",
    "    train_set.append(i)\n",
    "\n",
    "results = LR_on_first_n(dir_name, n, train_set)\n",
    "pred_labels = results[0]\n",
    "act_labels = results[1]\n",
    "coefs = results[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
